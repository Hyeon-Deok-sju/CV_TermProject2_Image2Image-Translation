{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f28f665-a5ee-4eb7-b40c-b405fc2d813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import functools\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "\n",
    "from collections import OrderedDict\n",
    "from subprocess import call\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision import transforms as tf\n",
    "from torchvision import models\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cdac5c5-b945-4428-ab2a-a8ee4da69f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c51c9197-7f7f-4c21-babd-07cf024fa33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\2021\\2학기 수업\\CV\\Pix2Pix\\hhd\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"D:/2021/2학기 수업/CV/Pix2Pix/hhd/\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb3cf2db-09e4-4bd5-a0a8-4bcabe5832d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_dir = './T2R_Dataset' #\n",
    "checkpoint_dir = './checkpoint'\n",
    "name = 'thermal2RGB' #\n",
    "\n",
    "gpu_ids = [0]\n",
    "\n",
    "print_freq = 100\n",
    "save_latest_freq = 1000\n",
    "save_epoch_freq = 10\n",
    "display_freq = 100\n",
    "\n",
    "load_size = 512 #\n",
    "crop_size = 256\n",
    "nThreads = 0 #\n",
    "no_flip = True #\n",
    "serial_batches = True #\n",
    "resize_or_crop = 'resize' #\n",
    "gray_only = False\n",
    "normalize = False # normalize input data\n",
    "is_train = True\n",
    "input_nc = 3\n",
    "output_nc = 3\n",
    "label_nc = 0\n",
    "norm = 'instance'\n",
    "\n",
    "no_lsgan = False\n",
    "no_l1_loss = False\n",
    "no_vgg_loss = False\n",
    "no_ganFeat_loss = False\n",
    "no_gan_loss = False\n",
    "\n",
    "pool_size = 0\n",
    "niter_decay = 50\n",
    "niter = 100\n",
    "lambda_feat = 10.0\n",
    "ndf = 64\n",
    "nef = 16\n",
    "ngf = 64\n",
    "n_layers_D = 3\n",
    "num_D = 2\n",
    "\n",
    "train_epochs = 10\n",
    "batch_size = 4\n",
    "lr = 0.0005\n",
    "\n",
    "continue_train = False\n",
    "load_pretrain = ''\n",
    "which_epoch = 'latest' #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0674992f-6174-4a77-a425-97e6cd409c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최소공배수\n",
    "def lcm(a, b):\n",
    "    return abs(a * b) / math.gcd(a, b) if a and b else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b8d82f3-43a8-4610-8153-cff5af97aabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoint\\thermal2RGB\\iter.txt\n"
     ]
    }
   ],
   "source": [
    "iter_path = os.path.join(checkpoint_dir, name, 'iter.txt')\n",
    "print(iter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23df4899-e2d1-4801-b112-2bb804ebb16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if continue_train:\n",
    "    try:\n",
    "        start_epoch, epoch_iter = np.loadtxt(iter_path, delimiter = ',', dtype = int)\n",
    "    except:\n",
    "        start_epoch, epoch_iter = 1, 0\n",
    "else:\n",
    "    start_epoch, epoch_iter = 1, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75611207-a18d-4296-b546-b457bd4b6718",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_freq = lcm(print_freq, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1e2a80e-5ee4-475d-9205-50d3c5e0c813",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTENSIONS = [\n",
    "    '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
    "    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP', '.tiff'\n",
    "]\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef3b2ace-8e58-4e6f-a470-9a163bc4744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(in_dir):\n",
    "    images = []\n",
    "    assert os.path.isdir(in_dir)\n",
    "    \n",
    "    for root, _, fnames in sorted(os.walk(in_dir)):\n",
    "        for fname in fnames:\n",
    "            if is_image_file(fname):\n",
    "                path = os.path.join(root, fname)\n",
    "                images.append(path)\n",
    "                \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a90a44b-6527-4b16-9cfd-44e97bb5f0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_ther_color_pil(ther_img_path):\n",
    "    ther_img = np.asarray(Image.open(ther_img_path))\n",
    "    if len(ther_img.shape) == 3:\n",
    "        ther_img = ther_img[:,:,0]\n",
    "    ther_img = np.stack([ther_img, ther_img, ther_img], -1)\n",
    "    ther_img = Image.fromarray(ther_img)\n",
    "\n",
    "    return ther_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a39c913-ff30-4a81-826a-337acf315590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(size):\n",
    "    w, h = size\n",
    "    new_h = h\n",
    "    new_w = w\n",
    "    if resize_or_crop == 'resize_and_crop':\n",
    "        new_h = new_w = load_size\n",
    "    elif resize_or_crop == 'scale_width_and_crop':\n",
    "        new_w = load_size\n",
    "        new_h = load_size * h // w\n",
    "    \n",
    "    x = random.randint(0, np.maximum(0, new_w - crop_size))\n",
    "    y = random.randint(0, np.maximum(0, new_h - crop_size))\n",
    "    \n",
    "    flip = random.random() > 0.5\n",
    "    return {'crop_pos': (x, y), 'flip': flip}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb825c9a-817a-4f13-99b5-b84a4a308ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(params, normalize = False, method = Image.BICUBIC):\n",
    "    transform_list = []\n",
    "    if 'resize' in resize_or_crop:\n",
    "        osize = [load_size, load_size]\n",
    "        transform_list.append(tf.Resize(osize, method))\n",
    "    elif 'scale_width' in resize_or_crop:\n",
    "        transform_list.append(tf.Lambda(lambda img: __scale_width(img, load_size, method)))\n",
    "        \n",
    "    if 'crop' in resize_or_crop:\n",
    "        transform_list.append(tf.Lambda(lambda img: __crop(img, params['crop_pos'], crop_size)))\n",
    "        \n",
    "    if is_train and not no_flip:\n",
    "        transform_list.append(tf.Lambda(lambda img: __flip(img, params['flip'])))\n",
    "        \n",
    "    transform_list += [tf.ToTensor()]\n",
    "    \n",
    "    if normalize:\n",
    "        transform_list += [tf.Normalize((0.5, 0.5, 0.5),\n",
    "                                        (0.5, 0.5, 0.5))]\n",
    "    return tf.Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fa5ca98-294a-40d5-91fe-c247e8b09bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(data.Dataset):\n",
    "    def __init__(self):\n",
    "        super(BaseDataset, self).__init__()\n",
    "\n",
    "    def initialize(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3e3cb1d-b1b1-481f-a3de-b9e3769f7039",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(BaseDataset):\n",
    "    def initialize(self, data_root_dir, is_train):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.root = data_root_dir\n",
    "        \n",
    "        dir_A = '_A'\n",
    "        self.dir_A = os.path.join(data_root_dir, 'train'+dir_A if is_train else 'test'+dir_A)\n",
    "        self.A_paths = sorted(make_dataset(self.dir_A))\n",
    "                \n",
    "        self.A_paths = [path for path in self.A_paths if not 'ipynb_checkpoints' in path]\n",
    "        \n",
    "        if is_train:\n",
    "            dir_B = '_B'\n",
    "            self.dir_B = os.path.join(data_root_dir, 'train'+dir_B)\n",
    "            self.B_paths = sorted(make_dataset(self.dir_B))\n",
    "        \n",
    "        self.dataset_size = len(self.A_paths)\n",
    "        \n",
    "        self.thm_gamma_low = 0.5\n",
    "        self.thm_gamma_high = 1.5\n",
    "        \n",
    "        contrast_param = (0.3, 1)\n",
    "        self.colorjitter1 = tf.ColorJitter(contrast = contrast_param)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        A_path = self.A_paths[index]\n",
    "        \n",
    "        if gray_only:\n",
    "            A = Image.open(A_path)\n",
    "        else:\n",
    "            A = gen_ther_color_pil(A_path)\n",
    "        params = get_params(A.size)\n",
    "        #print(A)\n",
    "        transform_A = get_transform(params, normalize)\n",
    "        A_tensor = transform_A(A)\n",
    "        A_tensor = self.colorjitter1(A_tensor)\n",
    "        \n",
    "        if is_train:\n",
    "            thm_random_gamma = np.random.uniform(self.thm_gamma_low, self.thm_gamma_high)\n",
    "            A_tensor = A_tensor ** thm_random_gamma\n",
    "            A_tensor = torch.clamp(A_tensor,0,1)\n",
    "            \n",
    "            B_path = self.B_paths[index]\n",
    "            \n",
    "            B = Image.open(B_path)\n",
    "            \n",
    "            transform_B = get_transform(params, normalize)\n",
    "            B_tensor = transform_B(B)\n",
    "            \n",
    "            input_dict = {'label': A_tensor, 'image': B_tensor, 'path': A_path}\n",
    "        else:\n",
    "            input_dict = {'label': A_tensor, 'path': A_path}\n",
    "        \n",
    "        return input_dict\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.A_paths) // batch_size * batch_size\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6a0640c-d082-4f8e-8739-3d79ea75c75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataLoader():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def initialize(self):\n",
    "        pass\n",
    "\n",
    "    def load_data():\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de2d1e69-baa2-4312-9978-49abea5f5749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDataset(data_root_dir, is_train):\n",
    "    dataset = None\n",
    "    dataset = CustomDataset()\n",
    "    dataset.initialize(data_root_dir, is_train)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6122b51a-b553-41bf-9fba-8d23280cf7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetDataLoader(BaseDataLoader):    \n",
    "    def initialize(self, data_root_dir, is_train):\n",
    "        BaseDataLoader.initialize(self)\n",
    "        self.dataset = CreateDataset(data_root_dir, is_train)\n",
    "        self.dataloader = torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=not serial_batches,\n",
    "            num_workers=int(nThreads))\n",
    "\n",
    "    def load_data(self):\n",
    "        return self.dataloader\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2cea44c-fe56-41dd-8d0c-80c2c0c832c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDataLoader(data_root_dir, is_train):\n",
    "    data_loader = CustomDatasetDataLoader()\n",
    "    data_loader.initialize(data_root_dir, is_train)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f87a9e07-c9af-454e-abb4-1c895fcd922a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#training images = 2660\n"
     ]
    }
   ],
   "source": [
    "data_loader = CreateDataLoader(data_root_dir, is_train)\n",
    "dataset = data_loader.load_data()\n",
    "dataset_size = len(data_loader)\n",
    "print('#training images = %d' % dataset_size)\n",
    "#print(train_loader_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f612743a-a1f3-423c-b892-87c2c0270bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vgg19(torch.nn.Module):\n",
    "    def __init__(self, requires_grad=False):\n",
    "        super(Vgg19, self).__init__()\n",
    "        vgg_pretrained_features = models.vgg19(pretrained=True).features\n",
    "        self.slice1 = torch.nn.Sequential()\n",
    "        self.slice2 = torch.nn.Sequential()\n",
    "        self.slice3 = torch.nn.Sequential()\n",
    "        self.slice4 = torch.nn.Sequential()\n",
    "        self.slice5 = torch.nn.Sequential()\n",
    "        for x in range(2):\n",
    "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(2, 7):\n",
    "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(7, 12):\n",
    "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(12, 21):\n",
    "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(21, 30):\n",
    "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        h_relu1 = self.slice1(X)\n",
    "        h_relu2 = self.slice2(h_relu1)        \n",
    "        h_relu3 = self.slice3(h_relu2)        \n",
    "        h_relu4 = self.slice4(h_relu3)        \n",
    "        h_relu5 = self.slice5(h_relu4)                \n",
    "        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8105c3e-89f2-4ce6-adca-97a439d81d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(torch.nn.Module):\n",
    "    def initialize(self, is_train):\n",
    "        self.isTrain = is_train\n",
    "        self.Tensor = torch.Tensor\n",
    "        self.save_dir = os.path.join(checkpoint_dir, name)\n",
    "    \n",
    "    def set_input(self, input):\n",
    "        self.input = input\n",
    "    \n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    def test(self):\n",
    "        pass\n",
    "    \n",
    "    def get_image_paths(self):\n",
    "        pass\n",
    "    \n",
    "    def optimize_parameters(self):\n",
    "        pass\n",
    "    \n",
    "    def get_current_visuals(self):\n",
    "        return self.input\n",
    "    \n",
    "    def get_current_errors(self):\n",
    "        return {}\n",
    "    \n",
    "    def save(self, label):\n",
    "        pass\n",
    "    \n",
    "    def save_network(self, network, network_label, epoch_label):\n",
    "        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n",
    "        save_path = os.path.join(self.save_dir, save_filename)\n",
    "        torch.save(network.cpu().state_dict(), save_path)\n",
    "        if torch.cuda.is_available():\n",
    "            network.to(device)\n",
    "    \n",
    "    def load_network(self, network, network_label, epoch_label, save_dir = ''):\n",
    "        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n",
    "        \n",
    "        if not save_dir:\n",
    "            save_dir = self.save_dir\n",
    "        save_path = os.path.join(save_dir, save_filename)\n",
    "        if not os.path.isfile(save_path):\n",
    "            print('%s not exist!' % save_path)\n",
    "            if network_label == 'G':\n",
    "                raise('Generator must exist!')\n",
    "        else:\n",
    "            try:\n",
    "                network.load_state_dict(torch.load(save_path))\n",
    "            except:\n",
    "                pretrained_dict = torch.load(save_path)\n",
    "                model_dict = network.state_dict()\n",
    "                try:\n",
    "                    preparams = {}\n",
    "                    for pre_k, model_k in zip(pretrained_dict.keys(), model_dict.keys()):\n",
    "                        preparams[model_k] = pretrained_dict[pre_k]\n",
    "                    \n",
    "                    network.load_state_dict(preparams)\n",
    "                except:\n",
    "                    print('Pretrained network %s has fewer layers; The following are not initialized:' % network_label)\n",
    "                    for k, v in pretrained_dict.items():\n",
    "                        if v.size() == model_dict[k].size():\n",
    "                            model_dict[k] = v\n",
    "                    \n",
    "                    if sys.version_info >= (3,0):\n",
    "                        not_initialized = set()\n",
    "                    else:\n",
    "                        from sets import Set\n",
    "                        not_initialized = Set()\n",
    "                    \n",
    "                    for k, v in model_dict.items():\n",
    "                        if k not in pretrained_dict or v.size() != pretrained_dict[k].size():\n",
    "                            not_initialized.add(k.split('.')[0])\n",
    "                    \n",
    "                    print(sorted(not_initialized))\n",
    "                    network.load_state_dict(model_dict)\n",
    "    \n",
    "    def update_learning_rate():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96997f26-ee89-4ea6-85e9-e3f21d4c1bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_layer(norm_type = 'instance'):\n",
    "    if norm_type == 'batch':\n",
    "        norm_layer = functools.partial(torch.nn.BatchNorm2d, affine=True)\n",
    "    elif norm_type == 'instance':\n",
    "        norm_layer = functools.partial(torch.nn.InstanceNorm2d, affine=False)\n",
    "    else:\n",
    "        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n",
    "    return norm_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b6bdad2-6d18-4189-b93e-868918c6db8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIGGRAPHGenerator(torch.nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, norm_layer = torch.nn.BatchNorm2d, use_noise = False, use_tanh = True):\n",
    "        super(SIGGRAPHGenerator, self).__init__()\n",
    "        self.input_nc = input_nc\n",
    "        self.output_nc = output_nc\n",
    "        self.use_noise = use_noise\n",
    "        use_bias = True\n",
    "        \n",
    "        # Conv1\n",
    "        model1=[torch.nn.Conv2d(input_nc, 64, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model1+=[torch.nn.ReLU(True),]\n",
    "        model1+=[torch.nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model1+=[torch.nn.ReLU(True),]\n",
    "        model1+=[norm_layer(64),]\n",
    "\n",
    "        # Conv2\n",
    "        model2=[torch.nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=use_bias),]\n",
    "        model2+=[torch.nn.ReLU(True),]\n",
    "        model2+=[torch.nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model2+=[torch.nn.ReLU(True),]\n",
    "        model2+=[norm_layer(128),]\n",
    "\n",
    "        # Conv3\n",
    "        model3=[torch.nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=use_bias),]\n",
    "        model3+=[torch.nn.ReLU(True),]\n",
    "        model3+=[torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model3+=[torch.nn.ReLU(True),]\n",
    "        model3+=[torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model3+=[torch.nn.ReLU(True),]\n",
    "        model3+=[norm_layer(256),]\n",
    "\n",
    "        # Conv4\n",
    "        model4=[torch.nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=use_bias),]\n",
    "        model4+=[torch.nn.ReLU(True),]\n",
    "        model4+=[torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model4+=[torch.nn.ReLU(True),]\n",
    "        model4+=[torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model4+=[torch.nn.ReLU(True),]\n",
    "        model4+=[norm_layer(512),]\n",
    "\n",
    "        # Conv5\n",
    "        model5=[torch.nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
    "        model5+=[torch.nn.ReLU(True),]\n",
    "        model5+=[torch.nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
    "        model5+=[torch.nn.ReLU(True),]\n",
    "        model5+=[torch.nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
    "        model5+=[torch.nn.ReLU(True),]\n",
    "        model5+=[norm_layer(512),]\n",
    "\n",
    "        # Conv6\n",
    "        model6=[torch.nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
    "        model6+=[torch.nn.ReLU(True),]\n",
    "        model6+=[torch.nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
    "        model6+=[torch.nn.ReLU(True),]\n",
    "        model6+=[torch.nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias),]\n",
    "        model6+=[torch.nn.ReLU(True),]\n",
    "        model6+=[norm_layer(512),]\n",
    "\n",
    "        # Conv7\n",
    "        model7=[torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model7+=[torch.nn.ReLU(True),]\n",
    "        model7+=[torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model7+=[torch.nn.ReLU(True),]\n",
    "        model7+=[torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model7+=[torch.nn.ReLU(True),]\n",
    "        model7+=[norm_layer(512),]\n",
    "\n",
    "        # Conv7\n",
    "        model8up=[torch.nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=use_bias)]\n",
    "\n",
    "        model3short8=[torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "\n",
    "        model8=[torch.nn.ReLU(True),]\n",
    "        model8+=[torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model8+=[torch.nn.ReLU(True),]\n",
    "        model8+=[torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model8+=[torch.nn.ReLU(True),]\n",
    "        model8+=[norm_layer(256),]\n",
    "\n",
    "        # Conv9\n",
    "        model9up=[torch.nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=use_bias),]\n",
    "\n",
    "        model2short9=[torch.nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]    \n",
    "\n",
    "        model9=[torch.nn.ReLU(True),]\n",
    "        model9+=[torch.nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "        model9+=[torch.nn.ReLU(True),]\n",
    "        model9+=[norm_layer(128),]\n",
    "\n",
    "        # Conv10\n",
    "        model10up=[torch.nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1, bias=use_bias),]\n",
    "\n",
    "        model1short10=[torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=use_bias),]\n",
    "\n",
    "        model10=[torch.nn.ReLU(True),]\n",
    "        model10+=[torch.nn.Conv2d(128, 128, kernel_size=3, dilation=1, stride=1, padding=1, bias=use_bias),]\n",
    "        model10+=[torch.nn.LeakyReLU(negative_slope=.2),]\n",
    "\n",
    "        # regression output\n",
    "        model_out=[torch.nn.Conv2d(128, self.output_nc, kernel_size=1, padding=0, dilation=1, stride=1, bias=use_bias),]\n",
    "        if(use_tanh):\n",
    "            model_out+=[torch.nn.Tanh()]\n",
    "\n",
    "        self.model1 = torch.nn.Sequential(*model1)\n",
    "        self.model2 = torch.nn.Sequential(*model2)\n",
    "        self.model3 = torch.nn.Sequential(*model3)\n",
    "        self.model4 = torch.nn.Sequential(*model4)\n",
    "        self.model5 = torch.nn.Sequential(*model5)\n",
    "        self.model6 = torch.nn.Sequential(*model6)\n",
    "        self.model7 = torch.nn.Sequential(*model7)\n",
    "        self.model8up = torch.nn.Sequential(*model8up)\n",
    "        self.model8 = torch.nn.Sequential(*model8)\n",
    "        self.model9up = torch.nn.Sequential(*model9up)\n",
    "        self.model9 = torch.nn.Sequential(*model9)\n",
    "        self.model10up = torch.nn.Sequential(*model10up)\n",
    "        self.model10 = torch.nn.Sequential(*model10)\n",
    "        self.model3short8 = torch.nn.Sequential(*model3short8)\n",
    "        self.model2short9 = torch.nn.Sequential(*model2short9)\n",
    "        self.model1short10 = torch.nn.Sequential(*model1short10)\n",
    "\n",
    "        self.model_out = torch.nn.Sequential(*model_out)\n",
    "\n",
    "    def forward(self, input_A):\n",
    "        \n",
    "        conv1_2 = self.model1(input_A)#(3,256,256) -> (64,256,256)\n",
    "        conv2_2 = self.model2(conv1_2)#(64,256,256) -> (128,128,128)\n",
    "        conv3_3 = self.model3(conv2_2)#(128,128,128) -> (256,64,64)\n",
    "        conv4_3 = self.model4(conv3_3)#(256,64,64) -> (512,32,32)\n",
    "        conv5_3 = self.model5(conv4_3)#(512,32,32) -> (512,32,32)\n",
    "        conv6_3 = self.model6(conv5_3)#(512,32,32) -> (512,32,32)\n",
    "        conv7_3 = self.model7(conv6_3)#(512,32,32) -> (512,32,32)\n",
    "\n",
    "        conv8_up = self.model8up(conv7_3) + self.model3short8(conv3_3) \n",
    "        conv8_3 = self.model8(conv8_up)\n",
    "\n",
    "        conv9_up = self.model9up(conv8_3) + self.model2short9(conv2_2)\n",
    "        conv9_3 = self.model9(conv9_up)\n",
    "        conv10_up = self.model10up(conv9_3) + self.model1short10(conv1_2)\n",
    "        conv10_2 = self.model10(conv10_up)\n",
    "        \n",
    "        out_reg = self.model_out(conv10_2)\n",
    "\n",
    "        return out_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c11a314-56c2-4c34-bd30-9110e96b2e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0caedd55-6ecc-49e3-83ce-aa73828e2b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_G(input_nc, output_nc, norm = 'instance', use_noise = False):\n",
    "    norm_layer = get_norm_layer(norm_type = norm)\n",
    "    netG = SIGGRAPHGenerator(input_nc, output_nc, norm_layer, use_noise = use_noise, use_tanh = True)\n",
    "    print(netG)\n",
    "    #netG.to(device)\n",
    "    #netG.apply(weights_init)\n",
    "    return netG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e94acea5-6e0e-4a94-ba42-4b562302bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLayerDiscriminator(torch.nn.Module):\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=torch.nn.BatchNorm2d, use_sigmoid=False, getIntermFeat=False):\n",
    "        super(NLayerDiscriminator, self).__init__()\n",
    "        self.getIntermFeat = getIntermFeat\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        kw = 4\n",
    "        padw = int(np.ceil((kw-1.0)/2))\n",
    "        sequence = [[torch.nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), torch.nn.LeakyReLU(0.2, True)]]\n",
    "\n",
    "        nf = ndf\n",
    "        for n in range(1, n_layers):\n",
    "            nf_prev = nf\n",
    "            nf = min(nf * 2, 512)\n",
    "            sequence += [[\n",
    "                torch.nn.Conv2d(nf_prev, nf, kernel_size=kw, stride=2, padding=padw),\n",
    "                norm_layer(nf), torch.nn.LeakyReLU(0.2, True)\n",
    "            ]]\n",
    "\n",
    "        nf_prev = nf\n",
    "        nf = min(nf * 2, 512)\n",
    "        sequence += [[\n",
    "            torch.nn.Conv2d(nf_prev, nf, kernel_size=kw, stride=1, padding=padw),\n",
    "            norm_layer(nf),\n",
    "            torch.nn.LeakyReLU(0.2, True)\n",
    "        ]]\n",
    "\n",
    "        sequence += [[torch.nn.Conv2d(nf, 1, kernel_size=kw, stride=1, padding=padw)]]\n",
    "\n",
    "        if use_sigmoid:\n",
    "            sequence += [[torch.nn.Sigmoid()]]\n",
    "\n",
    "        if getIntermFeat:\n",
    "            for n in range(len(sequence)):\n",
    "                setattr(self, 'model'+str(n), torch.nn.Sequential(*sequence[n]))\n",
    "        else:\n",
    "            sequence_stream = []\n",
    "            for n in range(len(sequence)):\n",
    "                sequence_stream += sequence[n]\n",
    "            self.model = torch.nn.Sequential(*sequence_stream)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.getIntermFeat:\n",
    "            res = [input]\n",
    "            for n in range(self.n_layers+2):\n",
    "                model = getattr(self, 'model'+str(n))\n",
    "                res.append(model(res[-1]))\n",
    "            return res[1:]\n",
    "        else:\n",
    "            return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70d06729-087e-4a4c-8416-cc5e77a447d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiscaleDiscriminator(torch.nn.Module):\n",
    "    def __init__(self, input_nc, ndf = 64, n_layers = 3, norm_layer = torch.nn.BatchNorm2d,\n",
    "                 use_sigmoid = False, num_D = 3, getIntermFeat = False):\n",
    "        super(MultiscaleDiscriminator, self).__init__()\n",
    "        self.num_D = num_D\n",
    "        self.n_layers = n_layers\n",
    "        self.getIntermFeat = getIntermFeat\n",
    "        \n",
    "        for i in range(num_D):\n",
    "            netD = NLayerDiscriminator(input_nc, ndf, n_layers, norm_layer, use_sigmoid, getIntermFeat)\n",
    "            #netD = netD.to(device)\n",
    "            if getIntermFeat:\n",
    "                for j in range(n_layers+2):\n",
    "                    setattr(self, 'scale'+str(i)+'_layer'+str(j), getattr(netD, 'model'+str(j)))                                   \n",
    "            else:\n",
    "                setattr(self, 'layer'+str(i), netD.model)\n",
    "        self.downsample = torch.nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n",
    "        \n",
    "    def singleD_forward(self, model, input):\n",
    "        if self.getIntermFeat:\n",
    "            result = [input]\n",
    "            for i in range(len(model)):\n",
    "                result.append(model[i](result[-1]))\n",
    "            return result[1:]\n",
    "        else:\n",
    "            return [model(input)]\n",
    "    \n",
    "    def forward(self, input):        \n",
    "        num_D = self.num_D\n",
    "        result = []\n",
    "        input_downsampled = input\n",
    "        for i in range(num_D):\n",
    "            if self.getIntermFeat:\n",
    "                model = [getattr(self, 'scale'+str(num_D-1-i)+'_layer'+str(j)) for j in range(self.n_layers+2)]\n",
    "            else:\n",
    "                model = getattr(self, 'layer'+str(num_D-1-i))\n",
    "\n",
    "            result.append(self.singleD_forward(model, input_downsampled))\n",
    "            if i != (num_D-1):\n",
    "                input_downsampled = self.downsample(input_downsampled)\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22b107dd-e8aa-4262-b3b8-92c652e6be42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_D(input_nc, ndf, n_layers_D, norm = 'instance', use_sigmoid = False, num_D = 1, getIntermFeat=False):\n",
    "    norm_layer = get_norm_layer(norm_type = norm)\n",
    "    netD = MultiscaleDiscriminator(input_nc, ndf, n_layers_D, norm_layer, use_sigmoid, num_D, getIntermFeat)\n",
    "    print(netD)\n",
    "    #netD.to(device)\n",
    "    #netD.apply(weights_init)\n",
    "    return netD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7cf14da8-db5c-477b-ae57-4b7022f07691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses\n",
    "class GANLoss(torch.nn.Module):\n",
    "    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0,\n",
    "                 tensor=torch.cuda.FloatTensor):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.real_label = target_real_label\n",
    "        self.fake_label = target_fake_label\n",
    "        self.real_label_var = None\n",
    "        self.fake_label_var = None\n",
    "        self.Tensor = tensor\n",
    "        if use_lsgan:\n",
    "            self.loss = torch.nn.MSELoss().to(device)\n",
    "        else:\n",
    "            self.loss =  torch.nn.BCEWithLogitsLoss().to(device) \n",
    "\n",
    "    def get_target_tensor(self, input, target_is_real):\n",
    "        target_tensor = None\n",
    "        if target_is_real:\n",
    "            create_label = ((self.real_label_var is None) or\n",
    "                            (self.real_label_var.numel() != input.numel()))\n",
    "            if create_label:\n",
    "                real_tensor = self.Tensor(input.size()).fill_(self.real_label)\n",
    "                self.real_label_var = Variable(real_tensor, requires_grad=False)\n",
    "            target_tensor = self.real_label_var\n",
    "        else:\n",
    "            create_label = ((self.fake_label_var is None) or\n",
    "                            (self.fake_label_var.numel() != input.numel()))\n",
    "            if create_label:\n",
    "                fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n",
    "                self.fake_label_var = Variable(fake_tensor, requires_grad=False)\n",
    "            target_tensor = self.fake_label_var\n",
    "        return target_tensor\n",
    "\n",
    "    def __call__(self, input, target_is_real):\n",
    "        if isinstance(input[0], list):\n",
    "            loss = 0\n",
    "            for input_i in input:\n",
    "                pred = input_i[-1]\n",
    "                target_tensor = self.get_target_tensor(pred, target_is_real)\n",
    "                loss += self.loss(pred, target_tensor)\n",
    "            return loss\n",
    "        else:            \n",
    "            target_tensor = self.get_target_tensor(input[-1], target_is_real)\n",
    "            return self.loss(input[-1], target_tensor)\n",
    "\n",
    "class VGGLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGLoss, self).__init__()        \n",
    "        self.vgg = Vgg19().to(device)\n",
    "        self.criterion = torch.nn.L1Loss().to(device)\n",
    "        self.weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]        \n",
    "\n",
    "    def forward(self, x, y):     \n",
    "        if x.shape[1]==1:\n",
    "            x=torch.cat([x,x,x],dim=1)\n",
    "        if y.shape[1]==1:\n",
    "            y=torch.cat([y,y,y],dim=1)\n",
    "        \n",
    "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n",
    "        loss = 0\n",
    "        for i in range(len(x_vgg)):\n",
    "            loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach())        \n",
    "        return loss\n",
    "\n",
    "class HuberLoss(torch.nn.Module):\n",
    "    def __init__(self, delta=.01):\n",
    "        super(HuberLoss, self).__init__()\n",
    "        self.delta=delta\n",
    "\n",
    "    def __call__(self, in0, in1):\n",
    "        mask = torch.zeros_like(in0)\n",
    "        mann = torch.abs(in0-in1)\n",
    "        eucl = .5 * (mann**2)\n",
    "        mask[...] = mann < self.delta\n",
    "\n",
    "        # loss = eucl*mask + self.delta*(mann-.5*self.delta)*(1-mask)\n",
    "        loss = eucl*mask/self.delta + (mann-.5*self.delta)*(1-mask)\n",
    "        return torch.sum(loss,dim=1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5c28fd4-5fa7-4190-a426-ebba5ac20b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePool():\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "        if self.pool_size > 0:\n",
    "            self.num_imgs = 0\n",
    "            self.images = []\n",
    "\n",
    "    def query(self, images):\n",
    "        if self.pool_size == 0:\n",
    "            return images\n",
    "        return_images = []\n",
    "        for image in images.data:\n",
    "            image = torch.unsqueeze(image, 0)\n",
    "            if self.num_imgs < self.pool_size:\n",
    "                self.num_imgs = self.num_imgs + 1\n",
    "                self.images.append(image)\n",
    "                return_images.append(image)\n",
    "            else:\n",
    "                p = random.uniform(0, 1)\n",
    "                if p > 0.5:\n",
    "                    random_id = random.randint(0, self.pool_size-1)\n",
    "                    tmp = self.images[random_id].clone()\n",
    "                    self.images[random_id] = image\n",
    "                    return_images.append(tmp)\n",
    "                else:\n",
    "                    return_images.append(image)\n",
    "        return_images = Variable(torch.cat(return_images, 0))\n",
    "        return return_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0bb2580e-2315-4ee8-8760-da56dc4cbe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pix2PixHDModel(BaseModel):\n",
    "    def init_loss_filter(self, use_gan_loss, use_gan_feat_loss, use_vgg_loss, use_l1_loss):\n",
    "        flags = (use_gan_loss, use_gan_feat_loss, use_vgg_loss, use_gan_loss, use_gan_loss, use_l1_loss)\n",
    "        def loss_filter(g_gan, g_gan_feat, g_vgg, d_real, d_fake, g_l1):\n",
    "            return [l for (l,f) in zip((g_gan,g_gan_feat,g_vgg,d_real,d_fake, g_l1),flags) if f]\n",
    "        return loss_filter\n",
    "    \n",
    "    def initialize(self, is_train):\n",
    "        BaseModel.initialize(self, is_train)\n",
    "        if resize_or_crop != 'none' or not is_train:\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "        self.isTrain = is_train\n",
    "        \n",
    "        # Generator network\n",
    "        netG_input_nc = input_nc\n",
    "        self.netG = define_G(netG_input_nc, output_nc, norm = norm, use_noise = False)\n",
    "        self.netG.apply(weights_init)\n",
    "        \n",
    "        # Discriminator network\n",
    "        if self.isTrain:\n",
    "            use_sigmoid = no_lsgan\n",
    "            netD_input_nc = input_nc + output_nc\n",
    "            self.netD = define_D(netD_input_nc, ndf, n_layers_D, norm, use_sigmoid, \n",
    "                                          num_D, not no_ganFeat_loss)\n",
    "            self.netD.apply(weights_init)\n",
    "        \n",
    "        ## Encoder\n",
    "        if not self.isTrain or continue_train:\n",
    "            pretrained_path = '' if not self.isTrain else load_pretrain\n",
    "            self.load_network(self.netG, 'G', which_epoch, pretrained_path)\n",
    "            if self.isTrain:\n",
    "                self.load_network(self.netD, 'D', which_epoch, pretrained_path)\n",
    "        \n",
    "        # set loss functions and optimizers\n",
    "        if self.isTrain:\n",
    "            self.fake_pool = ImagePool(pool_size)\n",
    "            self.old_lr = lr\n",
    "            \n",
    "            self.loss_filter = self.init_loss_filter(not no_gan_loss, not no_ganFeat_loss, not no_vgg_loss, not no_l1_loss)\n",
    "            self.criterionGAN = GANLoss(use_lsgan = not no_lsgan, tensor = torch.cuda.FloatTensor)\n",
    "            self.criterionFeat = torch.nn.L1Loss().to(device)\n",
    "            self.criterionSmoothL1 = HuberLoss(delta=1. / 110.0)\n",
    "            \n",
    "            if not no_vgg_loss:\n",
    "                self.criterionVGG = VGGLoss()\n",
    "            \n",
    "            self.loss_names = self.loss_filter('G_GAN','G_GAN_Feat','G_VGG','D_real', 'D_fake','G_L1')\n",
    "            \n",
    "            params = list(self.netG.parameters())\n",
    "            \n",
    "            self.optimizer_G = torch.optim.Adam(params, lr = lr, betas = (0.5, 0.999))\n",
    "            \n",
    "            params = list(self.netD.parameters())\n",
    "            self.optimizer_D = torch.optim.Adam(params, lr = lr, betas = (0.5, 0.999))\n",
    "            \n",
    "    def encode_input(self, label_map, real_image = None, infer = False):\n",
    "        input_label = label_map.data\n",
    "        input_label = Variable(input_label, volatile = infer)\n",
    "        \n",
    "        if real_image is not None:\n",
    "            real_image = Variable(real_image.data)\n",
    "            \n",
    "        return input_label, real_image\n",
    "    \n",
    "    def discriminate(self, input_label, test_image, use_pool=False):\n",
    "\n",
    "        input_concat = torch.cat((input_label, test_image.detach()), dim=1)\n",
    "        if use_pool:            \n",
    "            fake_query = self.fake_pool.query(input_concat)\n",
    "            return self.netD.forward(fake_query)\n",
    "        else:\n",
    "            return self.netD.forward(input_concat)\n",
    "        \n",
    "    def forward(self, label, image, infer=False):\n",
    "        # Encode Inputs\n",
    "        \n",
    "        input_label, real_image = self.encode_input(label, image)\n",
    "\n",
    "        # Fake Generation\n",
    "        input_concat = input_label\n",
    "\n",
    "        fake_image = self.netG.forward(input_concat)\n",
    "\n",
    "        # Fake Detection and Loss\n",
    "        loss_G_GAN=0\n",
    "        loss_D_real=0\n",
    "        loss_D_fake=0\n",
    "\n",
    "        if not no_gan_loss:\n",
    "            \n",
    "            # Fake Detection and Loss\n",
    "            pred_fake_pool = self.discriminate(input_label, fake_image, use_pool=True)\n",
    "            loss_D_fake = self.criterionGAN(pred_fake_pool, False)        \n",
    "\n",
    "            # Real Detection and Loss        \n",
    "            pred_real = self.discriminate(input_label, real_image)\n",
    "            loss_D_real = self.criterionGAN(pred_real, True)\n",
    "\n",
    "            # GAN loss (Fake Passability Loss)        \n",
    "            pred_fake = self.netD.forward(torch.cat((input_label, fake_image), dim=1))        \n",
    "            loss_G_GAN = self.criterionGAN(pred_fake, True)\n",
    "               \n",
    "        loss_G_L1 = 0            \n",
    "        loss_G_L1 = 10 * torch.mean(self.criterionSmoothL1(fake_image.type(torch.cuda.FloatTensor),\n",
    "                                                            real_image.type(torch.cuda.FloatTensor)))\n",
    "        # GAN feature matching loss\n",
    "        loss_G_GAN_Feat = 0\n",
    "        if not no_ganFeat_loss:\n",
    "            feat_weights = 4.0 / (n_layers_D + 1)\n",
    "            D_weights = 1.0 / num_D\n",
    "            for i in range(num_D):\n",
    "                for j in range(len(pred_fake[i])-1):\n",
    "                    loss_G_GAN_Feat += D_weights * feat_weights * \\\n",
    "                        self.criterionFeat(pred_fake[i][j], pred_real[i][j].detach()) * lambda_feat\n",
    "                   \n",
    "        # VGG feature matching loss\n",
    "        loss_G_VGG = 0\n",
    "        if not no_vgg_loss:\n",
    "            loss_G_VGG = self.criterionVGG(fake_image, real_image) * lambda_feat\n",
    "        \n",
    "        # Only return the fake_B image if necessary to save BW\n",
    "        return [ self.loss_filter( loss_G_GAN, loss_G_GAN_Feat, loss_G_VGG, loss_D_real, loss_D_fake, loss_G_L1), None if not infer else fake_image ]\n",
    "\n",
    "    def inference(self, label, image=None):\n",
    "        # Encode Inputs        \n",
    "        image = Variable(image) if image is not None else None\n",
    "        input_label, real_image = self.encode_input(label, image, infer=True)\n",
    "\n",
    "        # Fake Generation\n",
    "        input_concat = input_label        \n",
    "           \n",
    "        if torch.__version__.startswith('0.4'):\n",
    "            with torch.no_grad():\n",
    "                fake_image = self.netG.forward(input_concat)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                fake_image = self.netG.forward(input_concat)\n",
    "\n",
    "        return fake_image\n",
    "\n",
    "    def save(self, which_epoch):\n",
    "        self.save_network(self.netG, 'G', which_epoch)\n",
    "        self.save_network(self.netD, 'D', which_epoch)\n",
    "\n",
    "    def update_fixed_params(self):\n",
    "        # after fixing the global generator for a number of iterations, also start finetuning it\n",
    "        params = list(self.netG.parameters())\n",
    "          \n",
    "        self.optimizer_G = torch.optim.Adam(params, lr=lr, betas=(0.5, 0.999))\n",
    "        \n",
    "    def update_learning_rate(self):\n",
    "        lrd = lr / niter_decay\n",
    "        lr = self.old_lr - lrd        \n",
    "        for param_group in self.optimizer_D.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        for param_group in self.optimizer_G.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        self.old_lr = lr\n",
    "\n",
    "class InferenceModel(Pix2PixHDModel):\n",
    "    def forward(self, inp):\n",
    "        label, inst = inp\n",
    "        return self.inference(label, inst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f7c3e9e-dcf1-42bc-be7b-df6ec2c0c43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIGGRAPHGenerator(\n",
      "  (model1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  )\n",
      "  (model2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  )\n",
      "  (model3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  )\n",
      "  (model4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  )\n",
      "  (model5): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  )\n",
      "  (model6): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  )\n",
      "  (model7): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  )\n",
      "  (model8up): Sequential(\n",
      "    (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  )\n",
      "  (model8): Sequential(\n",
      "    (0): ReLU(inplace=True)\n",
      "    (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  )\n",
      "  (model9up): Sequential(\n",
      "    (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  )\n",
      "  (model9): Sequential(\n",
      "    (0): ReLU(inplace=True)\n",
      "    (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  )\n",
      "  (model10up): Sequential(\n",
      "    (0): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  )\n",
      "  (model10): Sequential(\n",
      "    (0): ReLU(inplace=True)\n",
      "    (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (model3short8): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (model2short9): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (model1short10): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (model_out): Sequential(\n",
      "    (0): Conv2d(128, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")\n",
      "MultiscaleDiscriminator(\n",
      "  (scale0_layer0): Sequential(\n",
      "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (scale0_layer1): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (scale0_layer2): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (scale0_layer3): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (scale0_layer4): Sequential(\n",
      "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "  )\n",
      "  (scale1_layer0): Sequential(\n",
      "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (scale1_layer1): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (scale1_layer2): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (scale1_layer3): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (scale1_layer4): Sequential(\n",
      "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "  )\n",
      "  (downsample): AvgPool2d(kernel_size=3, stride=2, padding=[1, 1])\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = InferenceModel()\n",
    "if is_train:\n",
    "    model = Pix2PixHDModel()\n",
    "model = model.to(device)\n",
    "model.initialize(is_train)\n",
    "for param in model.parameters():\n",
    "    param.data = param.data.type(torch.cuda.FloatTensor)\n",
    "    #print(param.data.type())\n",
    "#if is_train:\n",
    "    #model = torch.nn.DataParallel(model)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56b1f3fd-00ea-4093-9a0b-0bb809982686",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G, optimizer_D = model.optimizer_G, model.optimizer_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b2f32e5-7c6c-4cda-833b-687c33e34f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "total_steps = (start_epoch - 1) * dataset_size + epoch_iter\n",
    "print(total_steps)\n",
    "\n",
    "display_delta = total_steps % display_freq\n",
    "print_delta = total_steps % print_freq\n",
    "save_delta = total_steps % save_latest_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d24327-84bd-4ac8-8276-808b524b8b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/150 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(start_epoch, niter + niter_decay + 1)):\n",
    "    epoch_start_time = time.time()\n",
    "    if epoch != start_epoch:\n",
    "        epoch_iter = epoch_iter % dataset_size\n",
    "    for i, data in tqdm(enumerate(dataset, start=epoch_iter)):\n",
    "        if total_steps % print_freq == print_delta:\n",
    "            iter_start_time = time.time()\n",
    "        total_steps += batch_size\n",
    "        epoch_iter += batch_size\n",
    "        \n",
    "        save_fake = total_steps % display_freq == display_delta\n",
    "        #print(data['label'].type(), data['image'].type())\n",
    "        data['label'] = data['label'].to(device)\n",
    "        data['image'] = data['image'].to(device)\n",
    "        #print(data['label'].type(), data['image'].type())\n",
    "        \n",
    "        losses, generated = model(data['label'], data['image'], infer=save_fake)\n",
    "        \n",
    "        losses = [ torch.mean(x) if not isinstance(x, int) else x for x in losses ]\n",
    "        loss_dict = dict(zip(model.loss_names, losses))\n",
    "        \n",
    "        if not no_gan_loss:\n",
    "            loss_D = (loss_dict['D_fake'] + loss_dict['D_real']) * 0.5\n",
    "            loss_G = loss_dict['G_GAN'] + loss_dict.get('G_GAN_Feat',0) + loss_dict.get('G_VGG',0) + loss_dict.get('G_L1',0)\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "            loss_G.backward()          \n",
    "            optimizer_G.step()      \n",
    "\n",
    "            # update discriminator weights\n",
    "            optimizer_D.zero_grad()\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "            \n",
    "        if total_steps % print_freq == print_delta:\n",
    "            errors = {k: v.data.item() if not isinstance(v, int) else v for k, v in loss_dict.items()}            \n",
    "            t = (time.time() - iter_start_time) / print_freq\n",
    " \n",
    "        if total_steps % save_latest_freq == save_delta:\n",
    "            print('saving the latest model (epoch %d, total_steps %d)' % (epoch, total_steps))\n",
    "            model.save('latest')            \n",
    "            np.savetxt(iter_path, (epoch, epoch_iter), delimiter=',', fmt='%d')\n",
    "            \n",
    "        if epoch_iter >= dataset_size:\n",
    "            break\n",
    "            \n",
    "    iter_end_time = time.time()\n",
    "    print('End of epoch %d / %d \\t Time Taken: %d sec' %\n",
    "          (epoch, niter + niter_decay, time.time() - epoch_start_time))\n",
    "    \n",
    "    ### save model for this epoch\n",
    "    if epoch % save_epoch_freq == 0:\n",
    "        print('saving the model at the end of epoch %d, iters %d' % (epoch, total_steps))        \n",
    "        model.save('latest')\n",
    "        model.save(epoch)\n",
    "        np.savetxt(iter_path, (epoch+1, 0), delimiter=',', fmt='%d')\n",
    "        \n",
    "    ### linearly decay learning rate after certain iterations\n",
    "    if epoch > niter:\n",
    "        model.update_learning_rate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d77de95-5f52-4ecc-b1ca-c28e02ca5e09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
